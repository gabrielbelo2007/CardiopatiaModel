{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TATYbUm5dwpH"
      },
      "source": [
        "### Importações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svwr9VYVOPd-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "import joblib\n",
        "\n",
        "# Importações de ML\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, fbeta_score, make_scorer\n",
        "\n",
        "# Importações de Reamostragem\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14KuTctvd8_I"
      },
      "source": [
        "### Montar o Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-vCDAo8d3nr",
        "outputId": "191fc0d1-e1c9-439c-9108-76523e35a702"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "output_path = '/content/drive/MyDrive/PS-Ligia_Time16/'\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu429sOaeGn3"
      },
      "source": [
        "### Carregar Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jgR_xsveAjK"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Se você executou o notebook de EDA deste projeto, e portanto, gerou e salvou os\n",
        "datasets processados; Descomente este código abaixo e fique a vontade para\n",
        "utilizar os datasets que foram salvos no seu próprio Google Drive.\n",
        "\n",
        "Caso não tenha criado os datasets, utilize o trecho descomentado para carregar\n",
        "ele já pronto do nosso Drive.\n",
        "'''\n",
        "\n",
        "\n",
        "# datasets = {\n",
        "#     \"Base\": pd.read_csv(os.path.join(output_path, 'risco_cardiovascular_base.csv')),\n",
        "#     \"Features\": pd.read_csv(os.path.join(output_path, 'risco_cardiovascular_features.csv'))\n",
        "# }\n",
        "\n",
        "datasets = {\n",
        "    \"Base\": pd.read_csv(\"https://drive.google.com/uc?id=1CT0vw85_nr4SQb1M_M9gLwooFePXZ6FN\"),\n",
        "    \"Features\": pd.read_csv(\"https://drive.google.com/uc?id=142P-9xy-kRboRVpHj2t9OniS0DUF0QTp\")\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWzeawpBeKNc"
      },
      "source": [
        "### Definição de Modelos e Hiperparâmetros para GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSXr-wWSeNcV"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Aqui criamos o grid de todos modelos com seus respectivos hiperparametros que\n",
        "vão ser combinados no gridsearch para acharmos a melhor configuração.\n",
        "\n",
        "Com a pipeline do imblearn tambem é possivel adicionar resampler ou qualquer\n",
        "pré-processamento pro grid\n",
        "\n",
        "'''\n",
        "resampler = [\n",
        "    SMOTE(random_state=42),\n",
        "    RandomOverSampler(random_state=42),\n",
        "    RandomUnderSampler(random_state=42),\n",
        "    'passthrough'\n",
        "]\n",
        "\n",
        "models_config = {\n",
        "    'RandomForest': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'resampler': resampler,\n",
        "            'classifier__n_estimators': [50, 100, 200],\n",
        "            'classifier__max_depth': [None, 5, 10],\n",
        "            'classifier__min_samples_split': [2, 5]\n",
        "        }\n",
        "    },\n",
        "    'DecisionTree': {\n",
        "        'model': DecisionTreeClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'resampler': resampler,\n",
        "            'classifier__max_depth': [None, 3, 5, 10],\n",
        "            'classifier__criterion': ['gini', 'entropy'],\n",
        "            'classifier__min_samples_leaf': [1, 2, 4]\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chFHTsieeney"
      },
      "source": [
        "### Loop de Treinamento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFRRdGO-emIL",
        "outputId": "3d9f3cb0-a1eb-4a9c-af01-7ddc4fa75768"
      },
      "outputs": [],
      "source": [
        "model_save_path = os.path.join(output_path, 'RandomForest_DecisionTree_Models')\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "results_list = []\n",
        "\n",
        "f2_scorer = make_scorer(fbeta_score, beta=2, average='macro', pos_label=None)\n",
        "\n",
        "for ds_name, df in datasets.items():\n",
        "    print(f\"\\n>>> Processando Dataset: {ds_name}\")\n",
        "\n",
        "    # Separar X e y\n",
        "    X = df.drop('BP_Category', axis=1)\n",
        "    y = df['BP_Category']\n",
        "\n",
        "    bmi_order = [\"Underweight\", \"Normal Weight\", \"Overweight\", \"Obese\"]\n",
        "\n",
        "    # Identificar colunas\n",
        "    cat_cols = X.copy().drop('BMI Category', axis=1).select_dtypes(include=['object']).columns.tolist()\n",
        "    num_cols = X.select_dtypes(exclude=['object']).columns.tolist()\n",
        "\n",
        "    # Preprocessor\n",
        "    preprocessor = ColumnTransformer([\n",
        "        ('num', StandardScaler(), num_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
        "        (\"ordinal\", OrdinalEncoder(categories=[bmi_order]), ['BMI Category']),\n",
        "    ])\n",
        "\n",
        "    # Split Treino/Teste (80/20)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    for model_name, config in models_config.items():\n",
        "        print(f\"\\n--- Iniciando GridSearch para {model_name} ---\")\n",
        "\n",
        "        # Pipeline base com placeholder para o resampler\n",
        "        pipeline = ImbPipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('resampler', SMOTE()), # Placeholder que será substituído pelo Grid\n",
        "            ('classifier', config['model'])\n",
        "        ])\n",
        "\n",
        "        # Cross-Validation (10 Folds para dataset pequeno)\n",
        "        cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "        # Grid Search\n",
        "        grid = GridSearchCV(\n",
        "            pipeline,\n",
        "            config['params'],\n",
        "            cv=cv,\n",
        "            scoring={'f2_macro': f2_scorer, 'f1_macro': 'f1_macro', 'accuracy': 'accuracy'},\n",
        "            refit='f2_macro',\n",
        "            return_train_score=True,\n",
        "            n_jobs=-1,\n",
        "            verbose=1,\n",
        "            error_score='raise'\n",
        "        )\n",
        "\n",
        "        grid.fit(X_train, y_train)\n",
        "\n",
        "        cv_res = pd.DataFrame(grid.cv_results_)\n",
        "\n",
        "        # Métricas de Treino (do melhor modelo)\n",
        "        best_idx = grid.best_index_\n",
        "        train_f2 = cv_res.loc[best_idx, 'mean_train_f2_macro']\n",
        "        train_f1 = cv_res.loc[best_idx, 'mean_train_f1_macro']\n",
        "        train_acc = cv_res.loc[best_idx, 'mean_train_accuracy']\n",
        "\n",
        "        # Métricas de Teste\n",
        "        y_pred = grid.predict(X_test)\n",
        "        test_f2 = fbeta_score(y_test, y_pred, beta=2, average='macro')\n",
        "        test_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "        test_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        # Salvar Modelo no Drive\n",
        "        model_filename = f\"{model_name}_{ds_name}.pkl\"\n",
        "        joblib.dump(grid.best_estimator_, os.path.join(model_save_path, model_filename))\n",
        "\n",
        "        # No results_list.append, adicione:\n",
        "        results_list.append({\n",
        "            'Model': model_name,\n",
        "            'Dataset': ds_name,\n",
        "            'Best_Params': str(grid.best_params_),\n",
        "            'Train_F2_Macro': train_f2,\n",
        "            'Train_F1_Macro': train_f1,\n",
        "            'Train_Accuracy': train_acc,\n",
        "            'Test_F2_Macro': test_f2,\n",
        "            'Test_F1_Macro': test_f1,\n",
        "            'Test_Accuracy': test_acc,\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqblz9rge-0k"
      },
      "source": [
        "### Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "rJJe1OJSex0F",
        "outputId": "7f89d894-29fa-42eb-80d8-2662fa6f9266"
      },
      "outputs": [],
      "source": [
        "df_results = pd.DataFrame(results_list)\n",
        "\n",
        "metrics_save_path = os.path.join(output_path, \"Metrics\")\n",
        "os.makedirs(metrics_save_path, exist_ok=True)\n",
        "\n",
        "# Criar tabelas separadas por modelo\n",
        "for model_name in models_config.keys():\n",
        "    df_model = df_results[df_results['Model'] == model_name].copy()\n",
        "\n",
        "    # Salvar no Drive\n",
        "    filename = f\"resultado_final_{model_name}.csv\"\n",
        "    df_model.to_csv(os.path.join(metrics_save_path, filename), index=False)\n",
        "\n",
        "    print(f\"\\n--- Tabela Final: {model_name} ---\")\n",
        "    display(df_model[['Dataset', 'Train_F1_Macro', 'Train_Accuracy', 'Test_F1_Macro', 'Test_Accuracy', 'Best_Params']])\n",
        "\n",
        "print(f\"\\nProcesso concluído! Os arquivos foram salvos em: {metrics_save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5BBklvrgb0i"
      },
      "source": [
        "### Feature Importance (Exclusivo do RandomForest e DecisionTree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1gG5WpOXWkzf",
        "outputId": "95253614-8145-43df-e51c-a222b982d2b7"
      },
      "outputs": [],
      "source": [
        "# --- Célula: Visualização da Importância das Features ---\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "def gerar_graficos_importancia(results_list, model_save_path, output_path):\n",
        "    \"\"\"\n",
        "    Carrega os modelos salvos e gera gráficos de importância das features\n",
        "    para cada combinação de Modelo e Dataset.\n",
        "    \"\"\"\n",
        "    # Configuração visual\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "    for res in results_list:\n",
        "        model_name = res['Model']\n",
        "        ds_name = res['Dataset']\n",
        "\n",
        "        # Caminho do modelo salvo no Drive\n",
        "        path_modelo = os.path.join(model_save_path, f\"{model_name}_{ds_name}.pkl\")\n",
        "\n",
        "        if os.path.exists(path_modelo):\n",
        "            # Carregar o pipeline completo (inclui preprocessor e classifier)\n",
        "            pipeline = joblib.load(path_modelo)\n",
        "\n",
        "            # 1. Recuperar nomes das colunas (considerando o One-Hot Encoding)\n",
        "            # O get_feature_names_out() extrai os nomes gerados pelo ColumnTransformer\n",
        "            preprocessor = pipeline.named_steps['preprocessor']\n",
        "            feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "            # Limpar nomes (remover prefixos técnicos 'num__' e 'cat__')\n",
        "            feature_names = [name.split('__')[-1] for name in feature_names]\n",
        "\n",
        "            # 2. Extrair importâncias do classificador (RF ou DT)\n",
        "            classifier = pipeline.named_steps['classifier']\n",
        "            importances = classifier.feature_importances_\n",
        "\n",
        "            # 3. Criar DataFrame para organizar o Top 10\n",
        "            df_imp = pd.DataFrame({\n",
        "                'Feature': feature_names,\n",
        "                'Importance': importances\n",
        "            }).sort_values(by='Importance', ascending=False).head(10)\n",
        "\n",
        "            # 4. Criação do Gráfico\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            cores = 'viridis' if model_name == 'RandomForest' else 'magma'\n",
        "\n",
        "            ax = sns.barplot(\n",
        "                data=df_imp,\n",
        "                x='Importance',\n",
        "                y='Feature',\n",
        "                palette=cores,\n",
        "                hue='Feature',\n",
        "                legend=False\n",
        "            )\n",
        "\n",
        "            # Estilização\n",
        "            plt.title(f\"Top 10 Features: {model_name}\\nDataset: {ds_name}\", fontsize=14, fontweight='bold', pad=15)\n",
        "            plt.xlabel(\"Importância Relativa (Gini Importance)\", fontsize=12)\n",
        "            plt.ylabel(\"Atributos\", fontsize=12)\n",
        "\n",
        "            # Adicionar os valores numéricos nas pontas das barras para precisão\n",
        "            for i in ax.containers:\n",
        "                ax.bar_label(i, fmt='%.3f', padding=5)\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Salvar o gráfico no Drive\n",
        "            nome_arquivo = f\"feat_imp_{model_name}_{ds_name}.png\"\n",
        "            plt.savefig(os.path.join(metrics_save_path, nome_arquivo), dpi=300)\n",
        "            plt.show()\n",
        "            print(f\"✅ Gráfico de importância para {model_name} ({ds_name}) salvo com sucesso.\")\n",
        "        else:\n",
        "            print(f\"⚠️ Modelo {model_name}_{ds_name}.pkl não encontrado no caminho especificado.\")\n",
        "\n",
        "# Execução automática\n",
        "gerar_graficos_importancia(results_list, model_save_path, output_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
